{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2242482,
          "sourceType": "datasetVersion",
          "datasetId": 1347583
        }
      ],
      "dockerImageVersionId": 30301,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Library"
      ],
      "metadata": {
        "id": "Kh5iVzAHTXBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "Als8yBVjTXB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering"
      ],
      "metadata": {
        "id": "RZU_0HvBTXCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data lebih condong ke kanan"
      ],
      "metadata": {
        "id": "CC7Rdg0CTXCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modeling"
      ],
      "metadata": {
        "id": "srlxUSUFTXCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7F9CJkeYTXCq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iafgjr-jUo-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os # Added for path manipulation\n",
        "import kagglehub # Added for Kaggle dataset download\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# 1. Chargement (Simulé selon votre path)\n",
        "# Download latest version using kagglehub\n",
        "download_path = kagglehub.dataset_download(\"lihxlhx/give-me-some-credit\")\n",
        "\n",
        "print(\"Path to dataset files:\", download_path)\n",
        "\n",
        "# Load the training data from the downloaded path\n",
        "df = pd.read_csv(os.path.join(download_path, 'cs-training.csv'))\n",
        "# Pour l'exemple, supposons que 'df' est déjà chargé comme dans le PDF [cite: 27]\n",
        "\n",
        "# 2. Nettoyage (Basé sur le PDF [cite: 188-190])\n",
        "# Remplacement des valeurs manquantes par la médiane/mode\n",
        "df['NumberOfDependents'].fillna(df['NumberOfDependents'].mode()[0], inplace=True)\n",
        "df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)\n",
        "\n",
        "# 3. Sélection des Features (Les 10 colonnes demandées) et de la Cible\n",
        "features = [\n",
        "    'RevolvingUtilizationOfUnsecuredLines', 'age',\n",
        "    'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio',\n",
        "    'MonthlyIncome', 'NumberOfOpenCreditLinesAndLoans',\n",
        "    'NumberOfTimes90DaysLate', 'NumberRealEstateLoansOrLines',\n",
        "    'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfDependents'\n",
        "]\n",
        "target = 'SeriousDlqin2yrs'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# 4. Split Train/Test (Comme dans le PDF [cite: 297])\n",
        "# Séparation Train/Test\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Technique de l'Oversampling pour booster l'apprentissage du R-learning\n",
        "df_train = pd.concat([X_train_raw, y_train], axis=1)\n",
        "df_sain = df_train[df_train[target] == 0]\n",
        "df_detresse = df_train[df_train[target] == 1]\n",
        "\n",
        "# On multiplie les cas de détresse par 5 pour \"forcer\" l'agent à les voir\n",
        "df_detresse_upsampled = resample(df_detresse, replace=True, n_samples=len(df_detresse)*5, random_state=42)\n",
        "df_balanced = pd.concat([df_sain, df_detresse_upsampled]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "X_train_balanced = df_balanced[features]\n",
        "y_train_balanced = df_balanced[target].values\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
        "X_test_scaled = scaler.transform(X_test_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sTXH3U1Uoxk",
        "outputId": "15a1b8db-4d58-47ed-9fc0-17dd8b65c31d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/lihxlhx/give-me-some-credit?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.16M/5.16M [00:00<00:00, 46.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/lihxlhx/give-me-some-credit/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2168710799.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['NumberOfDependents'].fillna(df['NumberOfDependents'].mode()[0], inplace=True)\n",
            "/tmp/ipython-input-2168710799.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 2. L'AGENT AMÉLIORÉ (Weighted Rewards + Decay)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class ImprovedCreditRLAgent:\n",
        "    def __init__(self, learning_rate=0.001, epsilon_start=1.0, epsilon_end=0.01, decay_rate=0.99995):\n",
        "        # Gestion de l'exploration décroissante\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "        # Modèles linéaires (Q-function approximation)\n",
        "        # 'huber' loss est plus robuste aux outliers que 'squared_error'\n",
        "        self.q_model_0 = SGDRegressor(loss='huber', learning_rate='adaptive', eta0=learning_rate)\n",
        "        self.q_model_1 = SGDRegressor(loss='huber', learning_rate='adaptive', eta0=learning_rate)\n",
        "\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def _initialize_models(self, state_dim):\n",
        "        dummy_X = np.zeros((1, state_dim))\n",
        "        self.q_model_0.partial_fit(dummy_X, [0])\n",
        "        self.q_model_1.partial_fit(dummy_X, [0])\n",
        "        self.is_fitted = True\n",
        "\n",
        "    def select_action(self, state, training=True):\n",
        "        state = state.reshape(1, -1)\n",
        "        if not self.is_fitted:\n",
        "            self._initialize_models(state.shape[1])\n",
        "\n",
        "        # Epsilon-Greedy avec Decay\n",
        "        # Si on est en mode test (training=False), on exploite à 100%\n",
        "        current_epsilon = self.epsilon if training else 0.0\n",
        "\n",
        "        if np.random.rand() < current_epsilon:\n",
        "            return np.random.choice([0, 1])\n",
        "\n",
        "        q0 = self.q_model_0.predict(state)[0]\n",
        "        q1 = self.q_model_1.predict(state)[0]\n",
        "        return 0 if q0 > q1 else 1\n",
        "\n",
        "    def update(self, state, action, reward):\n",
        "        state = state.reshape(1, -1)\n",
        "        if not self.is_fitted:\n",
        "            self._initialize_models(state.shape[1])\n",
        "\n",
        "        # Mise à jour du modèle concerné\n",
        "        if action == 0:\n",
        "            self.q_model_0.partial_fit(state, [reward])\n",
        "        else:\n",
        "            self.q_model_1.partial_fit(state, [reward])\n",
        "\n",
        "        # Mise à jour de l'epsilon (Decay)\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.decay_rate)"
      ],
      "metadata": {
        "id": "ufENl9-eUo4d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 3. ENTRAÎNEMENT AVEC RÉCOMPENSES ASYMÉTRIQUES\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "agent = ImprovedCreditRLAgent(learning_rate=0.001) # Utilise la classe précédente\n",
        "cumulative_reward = 0\n",
        "\n",
        "print(f\"Entraînement sur {len(X_train_scaled)} échantillons (équilibrés)...\")\n",
        "\n",
        "for i in range(len(X_train_scaled)):\n",
        "    state = X_train_scaled[i]\n",
        "    true_label = y_train_balanced[i]\n",
        "\n",
        "    # Choix de l'action\n",
        "    action = agent.select_action(state, training=True)\n",
        "\n",
        "    # --- SYSTÈME DE RÉCOMPENSE CORRIGÉ ---\n",
        "    if action == true_label:\n",
        "        if true_label == 1:\n",
        "            reward = 100  # Récompense massive pour avoir trouvé un cas rare\n",
        "        else:\n",
        "            reward = 1    # Récompense de base\n",
        "    else:\n",
        "        if true_label == 1 and action == 0:\n",
        "            reward = -250 # PÉNALITÉ EXTRÊME : Ne surtout pas rater un défaut\n",
        "        else:\n",
        "            reward = -10  # Pénalité pour fausse alerte (augmentée pour éviter le spam de '1')\n",
        "\n",
        "    # Mise à jour\n",
        "    agent.update(state, action, reward)\n",
        "\n",
        "    if i % 20000 == 0:\n",
        "        print(f\"Progrès: {i}/{len(X_train_scaled)} | Epsilon: {agent.epsilon:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlQRL3opVXY4",
        "outputId": "5ccbce00-7b0a-4624-a9cb-020f3fb20708"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entraînement sur 133184 échantillons (équilibrés)...\n",
            "Progrès: 0/133184 | Epsilon: 1.000\n",
            "Progrès: 20000/133184 | Epsilon: 0.368\n",
            "Progrès: 40000/133184 | Epsilon: 0.135\n",
            "Progrès: 60000/133184 | Epsilon: 0.050\n",
            "Progrès: 80000/133184 | Epsilon: 0.018\n",
            "Progrès: 100000/133184 | Epsilon: 0.010\n",
            "Progrès: 120000/133184 | Epsilon: 0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_rl = []\n",
        "agent.epsilon = 0\n",
        "\n",
        "# Plus ce seuil est BAS, plus vous capturez de personnes en détresse\n",
        "# Essayez 0.10 ou 0.05\n",
        "custom_threshold = 0.35\n",
        "\n",
        "for i in range(len(X_test_scaled)):\n",
        "    state = X_test_scaled[i].reshape(1, -1)\n",
        "\n",
        "    q0 = agent.q_model_0.predict(state)[0]\n",
        "    q1 = agent.q_model_1.predict(state)[0]\n",
        "\n",
        "    # Transformation Softmax pour obtenir une probabilité entre 0 et 1\n",
        "    exp_q = np.exp([q0, q1] - np.max([q0, q1])) # Stabilité numérique\n",
        "    probs = exp_q / exp_q.sum()\n",
        "    prob_detresse = probs[1]\n",
        "\n",
        "    # Décision basée sur le seuil\n",
        "    if prob_detresse > custom_threshold:\n",
        "        action = 1\n",
        "    else:\n",
        "        action = 0\n",
        "\n",
        "    y_pred_rl.append(action)\n",
        "\n",
        "print(f\"\\n--- RÉSULTATS R-LEARNING (SEUIL PROBABILISTE: {custom_threshold}) ---\")\n",
        "print(confusion_matrix(y_test, y_pred_rl))\n",
        "print(classification_report(y_test, y_pred_rl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdbaRMsgVYDh",
        "outputId": "baf45c07-244a-47ef-d40b-8cce6175f4cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RÉSULTATS R-LEARNING (SEUIL PROBABILISTE: 0.35) ---\n",
            "[[41783   237]\n",
            " [ 2640   340]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.97     42020\n",
            "           1       0.59      0.11      0.19      2980\n",
            "\n",
            "    accuracy                           0.94     45000\n",
            "   macro avg       0.76      0.55      0.58     45000\n",
            "weighted avg       0.92      0.94      0.92     45000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NvBU6yjmVeEk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IgxqCeX-VfoK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yoK2dUfgVehG"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}